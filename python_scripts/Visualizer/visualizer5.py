#!/usr/bin/env python3
"""
REAL GPU-Accelerated Video Processor
===================================

This version ACTUALLY uses your GPUs with:
- Forced GPU operations (no CPU fallback)
- Large batch processing (256+ frames)
- GPU tensor operations throughout
- Optimized video decoding
- Real GPU memory utilization (8-12GB per GPU)

Target: 85-95% utilization on BOTH RTX 5060 Ti GPUs
"""

import json
import argparse
import logging
import sys
import os
import time
import cv2
import numpy as np
import pandas as pd
from pathlib import Path
from typing import Dict, List, Any, Tuple, Optional
from datetime import datetime
from collections import defaultdict
import warnings
import threading
import queue
from concurrent.futures import ThreadPoolExecutor, as_completed

# Critical GPU imports
try:
    import torch
    import torch.nn.functional as F
    from ultralytics import YOLO
    import torchvision.transforms as transforms
    
    # Force CUDA availability check
    if not torch.cuda.is_available():
        raise RuntimeError("CUDA not available!")
    
    TORCH_AVAILABLE = True
    print(f"‚úÖ CUDA available with {torch.cuda.device_count()} GPUs")
except ImportError as e:
    print(f"‚ùå PyTorch/YOLO import failed: {e}")
    sys.exit(1)

# Monitoring
try:
    import GPUtil
    import psutil
    MONITORING_AVAILABLE = True
except ImportError:
    MONITORING_AVAILABLE = False

# GPS
try:
    import gpxpy
    GPS_AVAILABLE = True
except ImportError:
    GPS_AVAILABLE = False

# Force GPU settings
torch.backends.cudnn.benchmark = True
torch.backends.cudnn.deterministic = False
os.environ["CUDA_LAUNCH_BLOCKING"] = "0"  # Async CUDA operations

warnings.filterwarnings('ignore')
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

class RealGPUProcessor:
    """REAL GPU-accelerated video processor that actually uses GPU memory and compute"""
    
    def __init__(self, config: Dict[str, Any]):
        self.config = config
        self.output_dir = Path(config['output_dir'])
        self.output_dir.mkdir(exist_ok=True)
        
        # Setup output directories
        for subdir in ['object_tracking', 'stoplight_detection', 'traffic_counting', 'processing_reports']:
            (self.output_dir / subdir).mkdir(exist_ok=True)
        
        # GPU setup
        self.gpu_ids = self.setup_real_gpus()
        self.models = {}
        self.load_models_on_gpus()
        
        # Performance settings
        self.batch_size = config.get('batch_size', 256)  # Much larger batches
        self.max_video_frames = config.get('max_video_frames', 10000)
        self.frame_skip = config.get('frame_skip', 2)  # Process every 2nd frame for speed
        
        # Statistics
        self.stats = {
            'processed_videos': 0,
            'failed_videos': 0,
            'total_frames': 0,
            'total_detections': 0,
            'gpu_utilization': {gpu_id: [] for gpu_id in self.gpu_ids}
        }
        
        logger.info(f"üöÄ Real GPU Processor initialized:")
        logger.info(f"   GPUs: {self.gpu_ids}")
        logger.info(f"   Batch size: {self.batch_size}")
        logger.info(f"   Max frames per video: {self.max_video_frames}")
    
    def setup_real_gpus(self) -> List[int]:
        """Setup and verify REAL GPU usage"""
        if not torch.cuda.is_available():
            raise RuntimeError("‚ùå CUDA not available!")
        
        gpu_count = torch.cuda.device_count()
        if gpu_count < 2:
            logger.warning(f"‚ö†Ô∏è Only {gpu_count} GPU(s) available. Expected 2 RTX 5060 Ti.")
        
        gpus = []
        for i in range(min(gpu_count, 2)):  # Use up to 2 GPUs
            try:
                torch.cuda.set_device(i)
                
                # Test GPU with actual tensor operation
                test_tensor = torch.randn(1000, 1000).cuda(i)
                result = torch.mm(test_tensor, test_tensor)
                del test_tensor, result
                torch.cuda.empty_cache()
                
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / (1024**3)
                
                logger.info(f"üéÆ GPU {i}: {gpu_name} ({gpu_memory:.1f}GB) - VERIFIED")
                gpus.append(i)
                
            except Exception as e:
                logger.error(f"‚ùå GPU {i} setup failed: {e}")
        
        if not gpus:
            raise RuntimeError("‚ùå No working GPUs found!")
        
        return gpus
    
    def load_models_on_gpus(self):
        """Load YOLO models on GPUs with forced GPU operations"""
        model_path = self.config.get('yolo_model', 'yolo11x.pt')
        
        for gpu_id in self.gpu_ids:
            try:
                logger.info(f"üîÑ Loading YOLO model on GPU {gpu_id}...")
                
                # Set device
                torch.cuda.set_device(gpu_id)
                device = f'cuda:{gpu_id}'
                
                # Load YOLO model
                model = YOLO(model_path)
                
                # FORCE model to GPU (critical!)
                model.model = model.model.to(device)
                model.model.eval()
                
                # Disable gradients for inference
                for param in model.model.parameters():
                    param.requires_grad = False
                
                # Warm up GPU with large batch
                logger.info(f"üî• Warming up GPU {gpu_id} with large batch...")
                warmup_batch = torch.randn(self.batch_size, 3, 640, 640).to(device)
                
                with torch.no_grad():
                    _ = model.model(warmup_batch)
                
                del warmup_batch
                torch.cuda.empty_cache()
                
                # Verify GPU memory usage
                memory_allocated = torch.cuda.memory_allocated(gpu_id) / (1024**3)
                memory_reserved = torch.cuda.memory_reserved(gpu_id) / (1024**3)
                
                logger.info(f"‚úÖ GPU {gpu_id} model loaded:")
                logger.info(f"   Memory allocated: {memory_allocated:.2f}GB")
                logger.info(f"   Memory reserved: {memory_reserved:.2f}GB")
                
                if memory_allocated < 1.0:
                    logger.warning(f"‚ö†Ô∏è GPU {gpu_id} using only {memory_allocated:.2f}GB - may not be on GPU!")
                
                self.models[gpu_id] = model
                
            except Exception as e:
                logger.error(f"‚ùå Failed to load model on GPU {gpu_id}: {e}")
                raise
    
    def extract_frames_fast(self, video_path: str) -> Tuple[np.ndarray, Dict]:
        """Extract frames efficiently with OpenCV optimizations"""
        cap = cv2.VideoCapture(video_path)
        if not cap.isOpened():
            return np.array([]), {}
        
        # Get video info
        fps = cap.get(cv2.CAP_PROP_FPS)
        frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))
        width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))
        height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))
        
        video_info = {
            'fps': fps, 'frame_count': frame_count,
            'width': width, 'height': height,
            'is_360': 1.8 <= (width/height) <= 2.2
        }
        
        # Optimize OpenCV
        cap.set(cv2.CAP_PROP_BUFFERSIZE, 1)
        
        # Smart frame sampling
        if frame_count > self.max_video_frames:
            step = max(1, frame_count // self.max_video_frames)
            frame_indices = list(range(0, frame_count, step))[:self.max_video_frames]
        else:
            frame_indices = list(range(0, frame_count, self.frame_skip))
        
        logger.info(f"üìπ Extracting {len(frame_indices)} frames from {frame_count} total")
        
        frames = []
        for frame_idx in frame_indices:
            cap.set(cv2.CAP_PROP_POS_FRAMES, frame_idx)
            ret, frame = cap.read()
            if ret:
                # Resize to YOLO input size (640x640) and convert to RGB
                frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
                frame_resized = cv2.resize(frame_rgb, (640, 640))
                frames.append(frame_resized)
        
        cap.release()
        
        if frames:
            # Convert to numpy array for batching
            frames_array = np.stack(frames).astype(np.float32) / 255.0
            frames_array = frames_array.transpose(0, 3, 1, 2)  # NHWC to NCHW
        else:
            frames_array = np.array([])
        
        video_info['extracted_frames'] = len(frames)
        video_info['frame_indices'] = frame_indices
        
        return frames_array, video_info
    
    def process_video_on_gpu(self, video_path: str, gpu_id: int, gps_df: pd.DataFrame) -> Dict:
        """Process entire video on specific GPU with MAXIMUM batch sizes"""
        video_name = Path(video_path).stem
        logger.info(f"üéÆ GPU {gpu_id} processing: {video_name}")
        
        start_time = time.time()
        device = f'cuda:{gpu_id}'
        model = self.models[gpu_id]
        
        try:
            # Extract frames
            frames_array, video_info = self.extract_frames_fast(video_path)
            if frames_array.size == 0:
                return {'status': 'failed', 'error': 'No frames extracted'}
            
            total_frames = len(frames_array)
            logger.info(f"üéÆ GPU {gpu_id}: Processing {total_frames} frames in batches of {self.batch_size}")
            
            all_detections = []
            
            # Process in large batches for MAXIMUM GPU utilization
            for i in range(0, total_frames, self.batch_size):
                batch_end = min(i + self.batch_size, total_frames)
                batch_frames = frames_array[i:batch_end]
                
                # Convert to GPU tensor
                batch_tensor = torch.from_numpy(batch_frames).to(device, non_blocking=True)
                
                # Log GPU memory usage for first batch
                if i == 0:
                    memory_before = torch.cuda.memory_allocated(gpu_id) / (1024**3)
                    logger.info(f"üéÆ GPU {gpu_id} memory before inference: {memory_before:.2f}GB")
                
                # YOLO inference on GPU
                with torch.no_grad():
                    torch.cuda.set_device(gpu_id)
                    results = model(batch_tensor, verbose=False)
                
                # Log GPU memory usage for first batch
                if i == 0:
                    memory_after = torch.cuda.memory_allocated(gpu_id) / (1024**3)
                    logger.info(f"üéÆ GPU {gpu_id} memory after inference: {memory_after:.2f}GB")
                    logger.info(f"üéÆ GPU {gpu_id} memory increase: {memory_after - memory_before:.2f}GB")
                
                # Process results
                for j, result in enumerate(results):
                    frame_idx = i + j
                    detections = self.extract_detections(result, frame_idx, gpu_id)
                    all_detections.append(detections)
                
                self.stats['total_frames'] += len(batch_frames)
                
                # Progress update for long videos
                if i > 0 and i % (self.batch_size * 5) == 0:
                    progress = (i / total_frames) * 100
                    logger.info(f"üéÆ GPU {gpu_id}: {progress:.1f}% complete")
                
                # Monitor GPU utilization
                if MONITORING_AVAILABLE and i % (self.batch_size * 3) == 0:
                    self.log_gpu_utilization()
            
            # Merge with GPS data
            final_results = self.merge_detections_with_gps(all_detections, gps_df, video_info)
            
            processing_time = time.time() - start_time
            fps = total_frames / processing_time if processing_time > 0 else 0
            
            logger.info(f"‚úÖ GPU {gpu_id} completed {video_name}:")
            logger.info(f"   Time: {processing_time:.2f}s")
            logger.info(f"   FPS: {fps:.1f}")
            logger.info(f"   Detections: {sum(len(d['detections']) for d in all_detections)}")
            
            self.stats['processed_videos'] += 1
            
            return {
                'status': 'success',
                'video_name': video_name,
                'processing_time': processing_time,
                'fps': fps,
                'gpu_id': gpu_id,
                'results': final_results
            }
            
        except Exception as e:
            logger.error(f"‚ùå GPU {gpu_id} error processing {video_path}: {e}")
            self.stats['failed_videos'] += 1
            return {'status': 'failed', 'error': str(e), 'gpu_id': gpu_id}
        
        finally:
            # Clear GPU memory
            torch.cuda.empty_cache()
    
    def extract_detections(self, result, frame_idx: int, gpu_id: int) -> Dict:
        """Extract detections from YOLO result"""
        detections = {
            'frame_idx': frame_idx,
            'detections': [],
            'counts': defaultdict(int),
            'gpu_id': gpu_id
        }
        
        if result.boxes is None or len(result.boxes) == 0:
            return detections
        
        boxes = result.boxes.xyxy.cpu().numpy()
        classes = result.boxes.cls.cpu().numpy().astype(int)
        confidences = result.boxes.conf.cpu().numpy()
        
        confidence_threshold = self.config.get('confidence_threshold', 0.3)
        
        for box, cls, conf in zip(boxes, classes, confidences):
            if conf >= confidence_threshold:
                obj_class = result.names.get(cls, 'unknown')
                
                detection = {
                    'bbox': box.tolist(),
                    'class': obj_class,
                    'confidence': float(conf),
                    'class_id': int(cls)
                }
                
                detections['detections'].append(detection)
                detections['counts'][obj_class] += 1
                self.stats['total_detections'] += 1
        
        return detections
    
    def merge_detections_with_gps(self, all_detections: List[Dict], gps_df: pd.DataFrame, 
                                 video_info: Dict) -> Dict:
        """Merge detections with GPS data"""
        results = {
            'object_tracking': [],
            'stoplight_detection': [],
            'traffic_counting': defaultdict(int)
        }
        
        fps = video_info.get('fps', 30)
        frame_indices = video_info.get('frame_indices', [])
        
        for detection_data in all_detections:
            frame_idx = detection_data['frame_idx']
            
            # Calculate actual timestamp
            if frame_idx < len(frame_indices):
                actual_frame_number = frame_indices[frame_idx]
                second = int(actual_frame_number / fps) if fps > 0 else frame_idx
            else:
                second = frame_idx
            
            # Get GPS data
            gps_row = gps_df[gps_df['second'] == second] if not gps_df.empty else pd.DataFrame()
            gps_data = gps_row.iloc[0] if not gps_row.empty else pd.Series()
            
            # Process detections
            for detection in detection_data['detections']:
                record = {
                    'frame_second': second,
                    'object_class': detection['class'],
                    'bbox': detection['bbox'],
                    'confidence': detection['confidence'],
                    'lat': float(gps_data.get('lat', 0)),
                    'lon': float(gps_data.get('lon', 0)),
                    'gps_time': str(gps_data.get('gpx_time', '')),
                    'gpu_id': detection_data['gpu_id'],
                    'video_type': '360¬∞' if video_info.get('is_360', False) else 'flat'
                }
                
                results['object_tracking'].append(record)
                
                # Traffic light detection
                if detection['class'] == 'traffic light':
                    stoplight_record = record.copy()
                    stoplight_record['stoplight_color'] = 'detected'
                    results['stoplight_detection'].append(stoplight_record)
            
            # Count objects
            for obj_class, count in detection_data['counts'].items():
                results['traffic_counting'][obj_class] += count
        
        return results
    
    def log_gpu_utilization(self):
        """Log current GPU utilization"""
        if MONITORING_AVAILABLE:
            try:
                gpus = GPUtil.getGPUs()
                for gpu in gpus:
                    if gpu.id in self.gpu_ids:
                        util = gpu.load * 100
                        memory_percent = (gpu.memoryUsed / gpu.memoryTotal) * 100
                        
                        logger.info(f"üéÆ GPU {gpu.id}: {util:.1f}% util, "
                                   f"{memory_percent:.1f}% mem ({gpu.memoryUsed}MB/{gpu.memoryTotal}MB)")
                        
                        self.stats['gpu_utilization'][gpu.id].append(util)
            except Exception as e:
                logger.debug(f"GPU monitoring error: {e}")
    
    def load_gps_data(self, gps_path: str) -> pd.DataFrame:
        """Load GPS data efficiently"""
        if not gps_path or not os.path.exists(gps_path):
            return pd.DataFrame()
        
        try:
            if gps_path.endswith('.csv'):
                df = pd.read_csv(gps_path)
                if 'long' in df.columns:
                    df['lon'] = df['long']
                return df
            elif gps_path.endswith('.gpx') and GPS_AVAILABLE:
                with open(gps_path, 'r') as f:
                    gpx = gpxpy.parse(f)
                
                records = []
                for track in gpx.tracks:
                    for segment in track.segments:
                        for i, point in enumerate(segment.points):
                            records.append({
                                'second': i,
                                'lat': point.latitude,
                                'lon': point.longitude,
                                'gpx_time': point.time
                            })
                
                return pd.DataFrame(records)
        except Exception as e:
            logger.warning(f"‚ö†Ô∏è GPS loading failed: {e}")
        
        return pd.DataFrame()
    
    def process_videos_dual_gpu(self, video_matches: Dict[str, Any]):
        """Process videos using both GPUs simultaneously"""
        total_videos = len(video_matches)
        logger.info(f"üöÄ Processing {total_videos} videos with dual GPU acceleration")
        
        # Start GPU monitoring
        monitor_thread = threading.Thread(target=self.monitor_gpu_usage, daemon=True)
        monitor_thread.start()
        
        # Prepare video list
        video_list = [
            {
                'path': video_path,
                'gps_path': info.get('gps_path', ''),
                'info': info
            }
            for video_path, info in video_matches.items()
        ]
        
        # Process videos with ThreadPoolExecutor for dual GPU
        max_workers = len(self.gpu_ids)  # One worker per GPU
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # Submit videos to GPUs
            future_to_video = {}
            
            for i, video_info in enumerate(video_list):
                gpu_id = self.gpu_ids[i % len(self.gpu_ids)]  # Round-robin GPU assignment
                gps_df = self.load_gps_data(video_info['gps_path'])
                
                future = executor.submit(self.process_video_on_gpu, video_info['path'], gpu_id, gps_df)
                future_to_video[future] = video_info
            
            # Collect results
            completed = 0
            for future in as_completed(future_to_video):
                video_info = future_to_video[future]
                
                try:
                    result = future.result()
                    
                    if result['status'] == 'success':
                        self.save_results(result)
                        completed += 1
                        progress = (completed / total_videos) * 100
                        
                        logger.info(f"üìä Progress: {progress:.1f}% ({completed}/{total_videos}) "
                                   f"- {result['video_name']} (GPU {result['gpu_id']}, {result['fps']:.1f} FPS)")
                    else:
                        logger.error(f"‚ùå Failed: {result.get('error', 'Unknown error')}")
                        
                except Exception as e:
                    logger.error(f"‚ùå Processing exception: {e}")
        
        # Generate final report
        self.generate_performance_report()
    
    def monitor_gpu_usage(self):
        """Monitor GPU usage in background"""
        while True:
            self.log_gpu_utilization()
            time.sleep(15)  # Monitor every 15 seconds
    
    def save_results(self, result: Dict):
        """Save video results to CSV"""
        video_name = result['video_name']
        results = result['results']
        
        # Object tracking
        if results['object_tracking']:
            df = pd.DataFrame(results['object_tracking'])
            output_file = self.output_dir / 'object_tracking' / f"{video_name}_tracking.csv"
            df.to_csv(output_file, index=False)
        
        # Stoplight detection
        if results['stoplight_detection']:
            df = pd.DataFrame(results['stoplight_detection'])
            output_file = self.output_dir / 'stoplight_detection' / f"{video_name}_stoplights.csv"
            df.to_csv(output_file, index=False)
        
        # Traffic counting
        if results['traffic_counting']:
            counting_data = [
                {'video_name': video_name, 'object_type': obj_type, 'total_count': count}
                for obj_type, count in results['traffic_counting'].items()
            ]
            df = pd.DataFrame(counting_data)
            output_file = self.output_dir / 'traffic_counting' / f"{video_name}_counts.csv"
            df.to_csv(output_file, index=False)
    
    def load_matcher50_results(self, results_path: str) -> Dict[str, Any]:
        """Load and filter matcher50 results for high-quality matches"""
        with open(results_path, 'r') as f:
            data = json.load(f)
        
        results = data.get('results', data)
        
        # Filter for high-quality matches
        min_score = self.config.get('min_score', 0.6)
        quality_map = {'excellent': 5, 'very_good': 4, 'good': 3, 'fair': 2, 'poor': 1}
        min_quality_level = quality_map.get(self.config.get('min_quality', 'good'), 3)
        
        filtered = {}
        for video_path, video_data in results.items():
            if 'matches' not in video_data or not video_data['matches']:
                continue
            
            best_match = video_data['matches'][0]
            score = best_match.get('combined_score', 0)
            quality = best_match.get('quality', 'poor')
            quality_level = quality_map.get(quality, 0)
            
            if score >= min_score and quality_level >= min_quality_level:
                filtered[video_path] = {
                    'gps_path': best_match.get('path', ''),
                    'quality': quality,
                    'score': score
                }
        
        logger.info(f"üîç Filtered to {len(filtered)} high-quality matches")
        return filtered
    
    def generate_performance_report(self):
        """Generate performance report"""
        report = {
            'summary': {
                'processed_videos': self.stats['processed_videos'],
                'failed_videos': self.stats['failed_videos'],
                'total_frames': self.stats['total_frames'],
                'total_detections': self.stats['total_detections']
            },
            'gpu_utilization': {
                f'gpu_{gpu_id}': {
                    'average_utilization': np.mean(utils) if utils else 0,
                    'max_utilization': np.max(utils) if utils else 0,
                    'samples': len(utils)
                }
                for gpu_id, utils in self.stats['gpu_utilization'].items()
            },
            'configuration': self.config
        }
        
        report_path = self.output_dir / 'processing_reports' / 'gpu_performance_report.json'
        with open(report_path, 'w') as f:
            json.dump(report, f, indent=2)
        
        logger.info(f"üèÅ GPU Processing Complete!")
        logger.info(f"   Videos: {self.stats['processed_videos']}")
        logger.info(f"   Frames: {self.stats['total_frames']:,}")
        logger.info(f"   Detections: {self.stats['total_detections']:,}")
        
        for gpu_id, utils in self.stats['gpu_utilization'].items():
            if utils:
                avg_util = np.mean(utils)
                logger.info(f"   GPU {gpu_id} avg utilization: {avg_util:.1f}%")

def main():
    parser = argparse.ArgumentParser(description="Real GPU-Accelerated Video Processor")
    
    parser.add_argument('-i', '--input', required=True, help='Matcher50 results JSON')
    parser.add_argument('-o', '--output', required=True, help='Output directory')
    parser.add_argument('--yolo-model', default='yolo11x.pt', help='YOLO model')
    parser.add_argument('--batch-size', type=int, default=256, help='GPU batch size (default: 256)')
    parser.add_argument('--max-video-frames', type=int, default=10000, help='Max frames per video')
    parser.add_argument('--frame-skip', type=int, default=2, help='Process every Nth frame')
    parser.add_argument('--min-score', type=float, default=0.6, help='Min match score')
    parser.add_argument('--min-quality', default='good', help='Min match quality')
    parser.add_argument('--confidence-threshold', type=float, default=0.3, help='YOLO confidence')
    
    args = parser.parse_args()
    
    if not Path(args.input).exists():
        logger.error(f"‚ùå Input file not found: {args.input}")
        sys.exit(1)
    
    config = {
        'output_dir': args.output,
        'yolo_model': args.yolo_model,
        'batch_size': args.batch_size,
        'max_video_frames': args.max_video_frames,
        'frame_skip': args.frame_skip,
        'min_score': args.min_score,
        'min_quality': args.min_quality,
        'confidence_threshold': args.confidence_threshold
    }
    
    logger.info("üöÄ Initializing REAL GPU-Accelerated Processor...")
    
    try:
        processor = RealGPUProcessor(config)
        video_matches = processor.load_matcher50_results(args.input)
        
        if not video_matches:
            logger.error("‚ùå No high-quality videos to process")
            sys.exit(1)
        
        processor.process_videos_dual_gpu(video_matches)
        
    except Exception as e:
        logger.error(f"‚ùå Processing failed: {e}")
        sys.exit(1)

if __name__ == "__main__":
    main()