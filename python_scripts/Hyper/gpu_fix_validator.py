#!/usr/bin/env python3
"""
GPU Fix Validation Tool

Validates that the GPU fixes were applied correctly to matcher49_gpu_fixed.py
and tests actual GPU utilization.

Usage:
    python gpu_fix_validator.py matcher49_gpu_fixed.py
"""

import re
import sys
import os
import torch
import time
import subprocess
import threading
from pathlib import Path
from typing import List, Dict, Tuple

class GPUFixValidator:
    """Validates GPU fixes and tests actual GPU utilization"""
    
    def __init__(self):
        self.validation_results = []
        self.gpu_test_results = []
        
    def validate_fixes(self, fixed_file: str) -> bool:
        """Validate that all GPU fixes were applied correctly"""
        print(f"üîç Validating GPU fixes in {fixed_file}...")
        
        if not os.path.exists(fixed_file):
            print(f"‚ùå File not found: {fixed_file}")
            return False
        
        with open(fixed_file, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # Run all validation checks
        self._check_gpu_manager_fixes(content)
        self._check_thread_pool_usage(content)
        self._check_tensor_placement(content)
        self._check_gpu_monitoring(content)
        self._check_gpu_verification(content)
        self._check_cpu_fallback_removal(content)
        
        # Print validation results
        self._print_validation_results()
        
        # Return overall validation status
        passed = all(result['passed'] for result in self.validation_results)
        return passed
    
    def test_gpu_utilization(self, gpu_ids: List[int] = [0, 1]) -> bool:
        """Test actual GPU utilization with the fixed code"""
        print(f"\nüéÆ Testing GPU utilization on GPUs {gpu_ids}...")
        
        if not torch.cuda.is_available():
            print("‚ùå CUDA not available for testing")
            return False
        
        available_gpus = torch.cuda.device_count()
        if max(gpu_ids) >= available_gpus:
            print(f"‚ùå Requested GPU IDs {gpu_ids} but only {available_gpus} GPUs available")
            return False
        
        # Test GPU memory allocation
        self._test_gpu_memory_allocation(gpu_ids)
        
        # Test GPU computation
        self._test_gpu_computation(gpu_ids)
        
        # Test multi-GPU coordination
        if len(gpu_ids) > 1:
            self._test_multi_gpu_coordination(gpu_ids)
        
        # Test GPU monitoring
        self._test_gpu_monitoring(gpu_ids)
        
        # Print test results
        self._print_gpu_test_results()
        
        return all(result['passed'] for result in self.gpu_test_results)
    
    def _check_gpu_manager_fixes(self, content: str):
        """Check GPU manager fixes"""
        checks = [
            {
                'name': 'acquire_gpu() method fixed',
                'pattern': r'def acquire_gpu\(self, timeout: int = 10\) -> Optional\[int\]:\s+"""FIXED: Reliable GPU acquisition that actually works"""',
                'required': True
            },
            {
                'name': 'release_gpu() method fixed',
                'pattern': r'def release_gpu\(self, gpu_id: int\):\s+"""FIXED: Reliable GPU release with proper cleanup"""',
                'required': True
            },
            {
                'name': 'GPU queue initialization fixed',
                'pattern': r'# Initialize GPU queue with all GPUs - FIXED',
                'required': True
            },
            {
                'name': 'GPU context verification',
                'pattern': r'with torch\.cuda\.device\(gpu_id\):.*?torch\.cuda\.synchronize\(gpu_id\)',
                'required': True
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content, re.DOTALL))
            self.validation_results.append({
                'category': 'GPU Manager',
                'check': check['name'],
                'passed': found,
                'required': check['required']
            })
    
    def _check_thread_pool_usage(self, content: str):
        """Check ThreadPoolExecutor usage"""
        checks = [
            {
                'name': 'ProcessPoolExecutor removed from imports',
                'pattern': r'from concurrent\.futures import.*ProcessPoolExecutor',
                'required': False,  # Should NOT be found
                'should_exist': False
            },
            {
                'name': 'ThreadPoolExecutor used for GPU work',
                'pattern': r'with ThreadPoolExecutor\(max_workers=',
                'required': True,
                'should_exist': True
            },
            {
                'name': 'No ProcessPoolExecutor usage',
                'pattern': r'ProcessPoolExecutor\(',
                'required': False,
                'should_exist': False
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content))
            expected = check.get('should_exist', True)
            passed = found == expected
            
            self.validation_results.append({
                'category': 'Threading',
                'check': check['name'],
                'passed': passed,
                'required': check['required']
            })
    
    def _check_tensor_placement(self, content: str):
        """Check tensor placement fixes"""
        checks = [
            {
                'name': 'Non-blocking tensor moves',
                'pattern': r'\.to\(device, non_blocking=True\)',
                'required': True
            },
            {
                'name': 'Device specification in tensor creation',
                'pattern': r'torch\.\w+\([^)]*device=device[^)]*\)',
                'required': True
            },
            {
                'name': 'GPU device verification',
                'pattern': r'if.*device\.type != [\'"]cuda[\'"]',
                'required': True
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content))
            self.validation_results.append({
                'category': 'Tensor Placement',
                'check': check['name'],
                'passed': found,
                'required': check['required']
            })
    
    def _check_gpu_monitoring(self, content: str):
        """Check GPU monitoring implementation"""
        checks = [
            {
                'name': 'GPUUtilizationMonitor class',
                'pattern': r'class GPUUtilizationMonitor:',
                'required': True
            },
            {
                'name': 'Real-time monitoring loop',
                'pattern': r'def _monitor_loop\(self\):',
                'required': True
            },
            {
                'name': 'GPU monitoring initialization',
                'pattern': r'gpu_monitor = GPUUtilizationMonitor',
                'required': True
            },
            {
                'name': 'Monitoring cleanup',
                'pattern': r'gpu_monitor\.stop_monitoring\(\)',
                'required': True
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content))
            self.validation_results.append({
                'category': 'GPU Monitoring',
                'check': check['name'],
                'passed': found,
                'required': check['required']
            })
    
    def _check_gpu_verification(self, content: str):
        """Check GPU verification system"""
        checks = [
            {
                'name': 'GPU verification function',
                'pattern': r'def verify_gpu_setup\(gpu_ids: List\[int\]\) -> bool:',
                'required': True
            },
            {
                'name': 'GPU testing with computation',
                'pattern': r'test_tensor = torch\.randn.*device=f[\'"]cuda:\{gpu_id\}[\'"]',
                'required': True
            },
            {
                'name': 'GPU verification call',
                'pattern': r'if not verify_gpu_setup\(args\.gpu_ids\):',
                'required': True
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content))
            self.validation_results.append({
                'category': 'GPU Verification',
                'check': check['name'],
                'passed': found,
                'required': check['required']
            })
    
    def _check_cpu_fallback_removal(self, content: str):
        """Check CPU fallback removal"""
        checks = [
            {
                'name': 'No silent CPU fallbacks',
                'pattern': r'return None.*?# GPU.*?fail',
                'required': False,
                'should_exist': False
            },
            {
                'name': 'Explicit GPU requirement errors',
                'pattern': r'raise RuntimeError.*GPU.*required',
                'required': True,
                'should_exist': True
            }
        ]
        
        for check in checks:
            found = bool(re.search(check['pattern'], content, re.DOTALL))
            expected = check.get('should_exist', True)
            passed = found == expected
            
            self.validation_results.append({
                'category': 'CPU Fallback Removal',
                'check': check['name'],
                'passed': passed,
                'required': check['required']
            })
    
    def _test_gpu_memory_allocation(self, gpu_ids: List[int]):
        """Test GPU memory allocation"""
        for gpu_id in gpu_ids:
            try:
                device = torch.device(f'cuda:{gpu_id}')
                
                # Test memory allocation
                initial_memory = torch.cuda.memory_allocated(gpu_id)
                test_tensor = torch.randn(2048, 2048, device=device)
                allocated_memory = torch.cuda.memory_allocated(gpu_id)
                
                memory_used = (allocated_memory - initial_memory) / (1024**2)  # MB
                
                # Clean up
                del test_tensor
                torch.cuda.empty_cache()
                
                self.gpu_test_results.append({
                    'category': 'Memory Allocation',
                    'test': f'GPU {gpu_id} memory allocation',
                    'passed': memory_used > 0,
                    'details': f'{memory_used:.1f}MB allocated'
                })
                
            except Exception as e:
                self.gpu_test_results.append({
                    'category': 'Memory Allocation',
                    'test': f'GPU {gpu_id} memory allocation',
                    'passed': False,
                    'details': f'Error: {e}'
                })
    
    def _test_gpu_computation(self, gpu_ids: List[int]):
        """Test GPU computation performance"""
        for gpu_id in gpu_ids:
            try:
                device = torch.device(f'cuda:{gpu_id}')
                
                # Create test matrices
                size = 2048
                a = torch.randn(size, size, device=device)
                b = torch.randn(size, size, device=device)
                
                # Time computation
                start_time = time.time()
                c = torch.matmul(a, b)
                torch.cuda.synchronize(gpu_id)
                end_time = time.time()
                
                computation_time = end_time - start_time
                gflops = (2 * size**3) / (computation_time * 1e9)
                
                # Clean up
                del a, b, c
                torch.cuda.empty_cache()
                
                self.gpu_test_results.append({
                    'category': 'GPU Computation',
                    'test': f'GPU {gpu_id} matrix multiplication',
                    'passed': gflops > 100,  # Expect at least 100 GFLOPS
                    'details': f'{gflops:.1f} GFLOPS, {computation_time:.3f}s'
                })
                
            except Exception as e:
                self.gpu_test_results.append({
                    'category': 'GPU Computation',
                    'test': f'GPU {gpu_id} computation',
                    'passed': False,
                    'details': f'Error: {e}'
                })
    
    def _test_multi_gpu_coordination(self, gpu_ids: List[int]):
        """Test multi-GPU coordination"""
        try:
            results = {}
            
            def gpu_worker(gpu_id):
                try:
                    device = torch.device(f'cuda:{gpu_id}')
                    data = torch.randn(1000, 1000, device=device)
                    result = torch.sum(data * data)
                    results[gpu_id] = result.cpu().item()
                except Exception as e:
                    results[gpu_id] = f'Error: {e}'
            
            # Run workers simultaneously
            threads = []
            for gpu_id in gpu_ids:
                thread = threading.Thread(target=gpu_worker, args=(gpu_id,))
                threads.append(thread)
                thread.start()
            
            # Wait for completion
            for thread in threads:
                thread.join(timeout=10)
            
            # Check results
            successful = sum(1 for v in results.values() if isinstance(v, float))
            
            self.gpu_test_results.append({
                'category': 'Multi-GPU',
                'test': 'Concurrent GPU processing',
                'passed': successful == len(gpu_ids),
                'details': f'{successful}/{len(gpu_ids)} GPUs successful'
            })
            
        except Exception as e:
            self.gpu_test_results.append({
                'category': 'Multi-GPU',
                'test': 'Multi-GPU coordination',
                'passed': False,
                'details': f'Error: {e}'
            })
    
    def _test_gpu_monitoring(self, gpu_ids: List[int]):
        """Test GPU monitoring capability"""
        try:
            # Check nvidia-smi availability
            result = subprocess.run(['nvidia-smi', '--query-gpu=utilization.gpu', '--format=csv,noheader,nounits'], 
                                  capture_output=True, text=True, check=True, timeout=5)
            
            utilizations = [int(line.strip()) for line in result.stdout.strip().split('\n') if line.strip().isdigit()]
            
            self.gpu_test_results.append({
                'category': 'GPU Monitoring',
                'test': 'nvidia-smi accessibility',
                'passed': len(utilizations) >= len(gpu_ids),
                'details': f'Utilizations: {utilizations}'
            })
            
        except Exception as e:
            self.gpu_test_results.append({
                'category': 'GPU Monitoring',
                'test': 'nvidia-smi accessibility',
                'passed': False,
                'details': f'Error: {e}'
            })
    
    def _print_validation_results(self):
        """Print validation results"""
        print(f"\nüìã VALIDATION RESULTS:")
        print(f"{'='*80}")
        
        categories = {}
        for result in self.validation_results:
            category = result['category']
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        total_passed = 0
        total_required = 0
        
        for category, results in categories.items():
            print(f"\nüîß {category}:")
            category_passed = 0
            category_required = 0
            
            for result in results:
                status = "‚úÖ" if result['passed'] else "‚ùå"
                required = " (REQUIRED)" if result['required'] else ""
                print(f"   {status} {result['check']}{required}")
                
                if result['passed']:
                    category_passed += 1
                if result['required']:
                    category_required += 1
                    total_required += 1
                    if result['passed']:
                        total_passed += 1
            
            print(f"   Category Status: {category_passed}/{len(results)} passed")
        
        print(f"\nüìä OVERALL VALIDATION:")
        print(f"   Required fixes: {total_passed}/{total_required} passed")
        print(f"   Status: {'‚úÖ PASSED' if total_passed == total_required else '‚ùå FAILED'}")
    
    def _print_gpu_test_results(self):
        """Print GPU test results"""
        print(f"\nüéÆ GPU TEST RESULTS:")
        print(f"{'='*80}")
        
        categories = {}
        for result in self.gpu_test_results:
            category = result['category']
            if category not in categories:
                categories[category] = []
            categories[category].append(result)
        
        total_passed = 0
        total_tests = len(self.gpu_test_results)
        
        for category, results in categories.items():
            print(f"\nüîß {category}:")
            category_passed = 0
            
            for result in results:
                status = "‚úÖ" if result['passed'] else "‚ùå"
                details = f" - {result['details']}" if 'details' in result else ""
                print(f"   {status} {result['test']}{details}")
                
                if result['passed']:
                    category_passed += 1
                    total_passed += 1
            
            print(f"   Category Status: {category_passed}/{len(results)} passed")
        
        print(f"\nüìä OVERALL GPU TESTING:")
        print(f"   Tests passed: {total_passed}/{total_tests}")
        print(f"   Status: {'‚úÖ READY FOR GPU PROCESSING' if total_passed == total_tests else '‚ö†Ô∏è ISSUES DETECTED'}")
        
        if total_passed == total_tests:
            print(f"\nüöÄ YOUR SYSTEM IS READY:")
            print(f"   ‚Ä¢ GPU fixes validated successfully")
            print(f"   ‚Ä¢ Both RTX 5060 Ti GPUs are working")
            print(f"   ‚Ä¢ Multi-GPU coordination functional")
            print(f"   ‚Ä¢ GPU monitoring operational")

def test_system_requirements():
    """Test basic system requirements"""
    print(f"üîç SYSTEM REQUIREMENTS CHECK:")
    print(f"{'='*80}")
    
    # Check PyTorch CUDA
    cuda_available = torch.cuda.is_available()
    print(f"PyTorch CUDA: {'‚úÖ' if cuda_available else '‚ùå'} {torch.version.cuda if cuda_available else 'Not available'}")
    
    if cuda_available:
        gpu_count = torch.cuda.device_count()
        print(f"GPU Count: {'‚úÖ' if gpu_count >= 2 else '‚ö†Ô∏è'} {gpu_count} GPUs detected")
        
        for i in range(gpu_count):
            props = torch.cuda.get_device_properties(i)
            memory_gb = props.total_memory / (1024**3)
            is_rtx = "RTX" in props.name.upper()
            print(f"  GPU {i}: {'üéÆ' if is_rtx else 'üì±'} {props.name} ({memory_gb:.1f}GB)")
    
    # Check nvidia-smi
    try:
        result = subprocess.run(['nvidia-smi', '--version'], capture_output=True, text=True, check=True, timeout=5)
        print(f"nvidia-smi: ‚úÖ Available")
    except:
        print(f"nvidia-smi: ‚ùå Not available")
    
    print(f"{'='*80}")

def main():
    """Main validation function"""
    if len(sys.argv) != 2:
        print("Usage: python gpu_fix_validator.py matcher49_gpu_fixed.py")
        print("\nThis tool validates that GPU fixes were applied correctly")
        print("and tests actual GPU utilization on your RTX 5060 Ti system.")
        sys.exit(1)
    
    fixed_file = sys.argv[1]
    
    print(f"üöÄ GPU FIX VALIDATION TOOL")
    print(f"{'='*80}")
    print(f"Target File: {fixed_file}")
    print(f"Expected Hardware: Dual RTX 5060 Ti GPUs")
    print(f"{'='*80}")
    
    # Test system requirements first
    test_system_requirements()
    
    # Create validator
    validator = GPUFixValidator()
    
    # Validate fixes
    fixes_valid = validator.validate_fixes(fixed_file)
    
    # Test GPU utilization if CUDA is available
    gpu_tests_passed = False
    if torch.cuda.is_available():
        available_gpus = list(range(min(2, torch.cuda.device_count())))
        if available_gpus:
            gpu_tests_passed = validator.test_gpu_utilization(available_gpus)
    else:
        print("\n‚ö†Ô∏è CUDA not available - skipping GPU utilization tests")
    
    # Final summary
    print(f"\nüéØ FINAL VALIDATION SUMMARY:")
    print(f"{'='*80}")
    print(f"Code Fixes: {'‚úÖ VALIDATED' if fixes_valid else '‚ùå ISSUES FOUND'}")
    
    if torch.cuda.is_available():
        print(f"GPU Tests: {'‚úÖ PASSED' if gpu_tests_passed else '‚ùå FAILED'}")
        
        if fixes_valid and gpu_tests_passed:
            print(f"\nüöÄ READY FOR HIGH-PERFORMANCE GPU PROCESSING!")
            print(f"   Your fixed matcher49_gpu_fixed.py should now:")
            print(f"   ‚Ä¢ Utilize both RTX 5060 Ti GPUs at 50-90%")
            print(f"   ‚Ä¢ Show real-time GPU monitoring")
            print(f"   ‚Ä¢ Process videos 10-50x faster")
            print(f"   ‚Ä¢ Fail fast if GPUs become unavailable")
            
            print(f"\nüìä RECOMMENDED COMMAND:")
            print(f"   python {Path(fixed_file).name} -d /path/to/data \\")
            print(f"      --gpu_ids 0 1 --turbo-mode --gpu_batch_size 64")
            
            print(f"\nüéÆ MONITOR GPU USAGE:")
            print(f"   watch -n 1 nvidia-smi")
        else:
            print(f"\n‚ö†Ô∏è ISSUES DETECTED - GPU processing may not work optimally")
    else:
        print(f"GPU Tests: ‚ö†Ô∏è SKIPPED (No CUDA)")
        print(f"\n‚ùå CUDA NOT AVAILABLE")
        print(f"   Install CUDA-enabled PyTorch:")
        print(f"   pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu121")
    
    print(f"{'='*80}")

if __name__ == "__main__":
    main()